{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqmvgv-0EOIu"
      },
      "outputs": [],
      "source": [
        "# 📄 CSV yükleyici\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AC7npZ26Ear"
      },
      "source": [
        "Chatbot Using Mistral-7B-Instruct-v0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3XVl-r3EEkZ",
        "outputId": "14a1e158-aeae-48bb-b6bf-25bb851427df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers sentence-transformers gradio accelerate\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
        "print(\"Current device:\", torch.cuda.current_device())\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# 1. Klinik notlarını yükle\n",
        "df = pd.read_csv(\"clinical_notes.csv\")\n",
        "df['not'] = df.apply(lambda row: f\"{row['hasta_adi']}, Etiket: {row['etiket']}. {row['not']}\", axis=1)\n",
        "\n",
        "\n",
        "# 2. Embedding modeli\n",
        "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "note_texts = df['not'].tolist()\n",
        "note_embeddings = embedder.encode(note_texts, convert_to_tensor=True)\n",
        "\n",
        "# 3. HF API uzerinden calisan LLM Modeli\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "\n",
        ")\n",
        "\n",
        "# 4. Cevap fonksiyonu\n",
        "def answer_question(question):\n",
        "    # Step 1: Embed the user question\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "\n",
        "    # Step 2: Semantic search over notes\n",
        "    hits = util.cos_sim(q_embed, note_embeddings)[0].topk(3)\n",
        "    relevant_notes = [note_texts[idx] for idx in hits[1]]\n",
        "\n",
        "    # Step 3: Check similarity score — are they actually related?\n",
        "    top_score = hits[0][0].item()\n",
        "    similarity_threshold = 0.4  # Tune if needed\n",
        "\n",
        "    if top_score < similarity_threshold:\n",
        "        # No relevant notes — use LLM as a general chatbot\n",
        "        print(\"⚠️ No relevant medical notes found — switching to open chat mode.\")\n",
        "\n",
        "        prompt = textwrap.dedent(f\"\"\"\\\n",
        "        Sen arkadaş canlısı bir yapay zeka asistanısın. Kullanıcının sorusuna veya mesajına doğal ve samimi bir şekilde cevap ver.\n",
        "\n",
        "        Kullanıcı: {question}\n",
        "        Asistan:\"\"\")\n",
        "\n",
        "    else:\n",
        "        # RAG: medical mode\n",
        "        context = \"\\n\".join([f\"- {note}\" for note in relevant_notes])\n",
        "\n",
        "        prompt = textwrap.dedent(f\"\"\"\\\n",
        "        Sen bir klinik yapay zeka asistanısın. Aşağıda bazı hasta notları var. Bu notlara dayanarak soruya açıklayıcı bir yanıt ver.\n",
        "\n",
        "        Soru: {question}\n",
        "        Klinik Notlar:\n",
        "        {context}\n",
        "\n",
        "        Yanıt:\"\"\")\n",
        "\n",
        "    # Step 4: Generate model response\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Step 5: Clean output\n",
        "    if \"Yanıt:\" in decoded:\n",
        "        response = decoded.split(\"Yanıt:\")[-1].strip()\n",
        "    elif \"Cevap:\" in decoded:\n",
        "        response = decoded.split(\"Cevap:\")[-1].strip()\n",
        "    else:\n",
        "        response = decoded.strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# 5. Gradio chat arayüzü\n",
        "gr.ChatInterface(\n",
        "    fn=lambda message, history: answer_question(message),\n",
        "    title=\"Klinik Notlara Dayalı AI Yanıtlayıcı\",\n",
        "    description=\"Yüklenen klinik notlara göre LLM destekli cevap üretir.\"\n",
        ").launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}